using Optim

# First, let's create the streamlined optimization function
function optimizeBlackHole_for_hypertuning(; learningRate, epochsPerIteration, numberOfCycles, totalTrainingPercent, true_parameters, initial_guess)
    """
    Streamlined version for hyperparameter optimization
    """
    
    try
        println("Testing: LR=$learningRate, epochs=$epochsPerIteration, cycles=$numberOfCycles, training%=$totalTrainingPercent")
        
        trainingFraction = totalTrainingPercent
        p_guess = pe_2_EL(initial_guess[1], initial_guess[2])
        
        true_p = true_parameters[1]
        true_e = true_parameters[2]
        
        # Generate training data
        trainingData = create_Schwarzschild_trainingData([true_p, true_e])
        timestep = 1
        true_solution = trainingData[2]
        
        # Set up Hamiltonian function (same as your original)
        function SchwarzschildHamiltonian_GENERIC(du, u, p, t)
            x = u
            function H(state_vec)
                t, r, θ, φ, p_t, p_r, p_θ, p_φ = state_vec
                NN_params = p.NN
                M = 1
                H_kepler = p_r^2/2 - M/r + p_φ^2/(2*r^2)
                NN_correction = NN([r, p_r, p_φ, p_t], NN_params, NN_state)[1][1]
                return H_kepler + NN_correction
            end
            
            grad_H = ForwardDiff.gradient(H, x)
            L = [zeros(4,4)  I(4); -I(4)  zeros(4,4)]
            du .= L * grad_H
        end
        
        # Neural network setup
        NN = Chain(Dense(4, 4, tanh), Dense(4, 4, tanh), Dense(4, 1))
        rng = MersenneTwister(42)
        NN_params, NN_state = Lux.setup(rng, NN)
        precision = Float64
        NN_params = Lux.fmap(x -> precision.(x), NN_params)
        
        # Weight initialization
        for layer in NN_params
            if ~isempty(layer)
                layer.weight .*= precision(1e-6) * randn(rng, size(layer.weight))
                layer.bias .*= precision(1e-6) * randn(rng, size(layer.bias))
            end
        end
        
        θ = (; NN = NN_params)
        θ = ComponentVector{precision}(θ)
        
        # Initial conditions
        R = initial_guess[1]/(1+initial_guess[2])
        E = p_guess[2]
        L = p_guess[3]
        u0 = [0, R, pi/2, 0, -1*E, 0, 0, L]
        tspan = (0.0, 2000.0)
        
        prob_learn = ODEProblem(SchwarzschildHamiltonian_GENERIC, u0, tspan, θ)
        
        # Loss function (simplified, no printing)
        function loss(pn, trainingFraction)
            newprob = remake(prob_learn, p = pn)
            sol = solve(newprob, RK4(), saveat=timestep)
            
            predicted_waveform_plus = compute_waveform(timestep, sol, 1.0; coorbital=true)[1]
            predicted_waveform_cross = compute_waveform(timestep, sol, 1.0; coorbital=true)[2]
            
            h_plus_training = trainingData[3]
            h_cross_training = trainingData[4]
            
            n_train = Int(floor(length(h_plus_training)*trainingFraction))
            n_pred = length(predicted_waveform_plus)
            n_compare = min(n_pred, n_train)
            
            loss_value = sum(abs2, predicted_waveform_plus[1:n_compare] .- h_plus_training[1:n_compare])
            loss_value += sum(abs2, predicted_waveform_cross[1:n_compare] .- h_cross_training[1:n_compare])
            loss_value /= n_compare
            
            return loss_value
        end
        
        # Simplified callback
        function callback_simple(pn, loss; dotrain = true)
            return false
        end
        
        # Training loop with reduced scope for hyperparameter search
        adtype = Optimization.AutoFiniteDiff()
        
        current_params = θ
        amountTrain = totalTrainingPercent / numberOfCycles
        
        for i in 1:numberOfCycles
            current_trainingFraction = i * amountTrain
            
            optf_current = Optimization.OptimizationFunction((x, p) -> loss(x, current_trainingFraction), adtype)
            optprob_current = Optimization.OptimizationProblem(optf_current, current_params)
            
            opt_result = Optimization.solve(optprob_current, Optim.BFGS(; initial_stepnorm = learningRate), 
                                          callback=callback_simple, maxiters=epochsPerIteration)
            
            current_params = opt_result.u
        end
        
        # Return final loss
        final_loss = loss(current_params, totalTrainingPercent)
        println("  -> Final loss: $final_loss")
        
        return final_loss
        
    catch e
        println("  -> FAILED with error: $e")
        return 1e6  # Return high penalty for failed runs
    end
end

# Option 1: Smart Random Search (Most Reliable)
function smart_random_hyperparameter_search(n_trials=15)
    """
    Intelligent random search that samples more densely around good regions
    """
    
    println("Starting smart random hyperparameter search with $n_trials trials...")
    
    best_loss = Inf
    best_params = nothing
    all_results = []
    
    # Define reasonable ranges
    lr_range = [1e-5, 1e-1]  # log scale
    epochs_range = [2, 8]
    cycles_range = [3, 10] 
    training_range = [0.05, 0.25]
    
    for i in 1:n_trials
        # Smart sampling - log scale for learning rate
        lr = 10^(rand() * (log10(lr_range[2]) - log10(lr_range[1])) + log10(lr_range[1]))
        epochs = rand(epochs_range[1]:epochs_range[2])
        cycles = rand(cycles_range[1]:cycles_range[2])
        training_pct = training_range[1] + rand() * (training_range[2] - training_range[1])
        
        params = (
            learning_rate = lr,
            epochs_per_iteration = epochs,
            number_of_cycles = cycles,
            training_percent = training_pct
        )
        
        println("\nTrial $i/$n_trials:")
        println("  LR: $(round(lr, sigdigits=3)), Epochs: $epochs, Cycles: $cycles, Training: $(round(training_pct, digits=3))")
        
        start_time = time()
        loss = optimizeBlackHole_for_hypertuning(
            learningRate = params.learning_rate,
            epochsPerIteration = params.epochs_per_iteration,
            numberOfCycles = params.number_of_cycles,
            totalTrainingPercent = params.training_percent,
            true_parameters = [10, 0.2],
            initial_guess = [10, 0.2]
        )
        elapsed = time() - start_time
        
        push!(all_results, (params = params, loss = loss, time = elapsed))
        
        if loss < best_loss
            best_loss = loss
            best_params = params
            println("  *** NEW BEST! Loss: $(round(best_loss, sigdigits=5)) ***")
        end
        
        println("  Completed in $(round(elapsed, digits=1)) seconds")
    end
    
    println("\n" * "="^60)
    println("HYPERPARAMETER SEARCH COMPLETED!")
    println("="^60)
    println("Best parameters:")
    println("  Learning rate: $(round(best_params.learning_rate, sigdigits=4))")
    println("  Epochs per iteration: $(best_params.epochs_per_iteration)")
    println("  Number of cycles: $(best_params.number_of_cycles)")
    println("  Training percent: $(round(best_params.training_percent, digits=3))")
    println("  Best loss: $(round(best_loss, sigdigits=5))")
    
    return best_params, best_loss, all_results
end

# Option 2: Optim.jl-based Continuous Optimization
function optim_hyperparameter_search()
    """
    Use Optim.jl to optimize hyperparameters as a continuous optimization problem
    """
    
    println("Starting Optim.jl-based hyperparameter optimization...")
    
    function objective_for_optim(x)
        # Transform the optimization variables to hyperparameters
        learning_rate = 10^(x[1])  # x[1] ∈ [-5, -1] -> lr ∈ [1e-5, 1e-1]
        training_percent = x[2]     # x[2] ∈ [0.05, 0.25]
        epochs_per_iteration = round(Int, x[3])  # x[3] ∈ [2, 8]
        number_of_cycles = round(Int, x[4])      # x[4] ∈ [3, 10]
        
        return optimizeBlackHole_for_hypertuning(
            learningRate = learning_rate,
            epochsPerIteration = epochs_per_iteration,
            numberOfCycles = number_of_cycles,
            totalTrainingPercent = training_percent,
            true_parameters = [10, 0.2],
            initial_guess = [10, 0.2]
        )
    end
    
    # Define bounds
    lower_bounds = [-5.0, 0.05, 2.0, 3.0]   # [log10(lr_min), training_min, epochs_min, cycles_min]
    upper_bounds = [-1.0, 0.25, 8.0, 10.0]  # [log10(lr_max), training_max, epochs_max, cycles_max]
    initial_guess = [-2.5, 0.15, 5.0, 6.0]  # Starting point
    
    # Run optimization
    result = optimize(objective_for_optim, lower_bounds, upper_bounds, initial_guess, Fminbox(BFGS()))
    
    # Extract best parameters
    best_x = Optim.minimizer(result)
    best_lr = 10^(best_x[1])
    best_training = best_x[2]
    best_epochs = round(Int, best_x[3])
    best_cycles = round(Int, best_x[4])
    
    println("\nOptim.jl optimization completed!")
    println("Best learning rate: $(round(best_lr, sigdigits=4))")
    println("Best training percent: $(round(best_training, digits=3))")
    println("Best epochs per iteration: $best_epochs")
    println("Best number of cycles: $best_cycles")
    println("Best loss: $(round(Optim.minimum(result), sigdigits=5))")
    
    return (learning_rate=best_lr, epochs_per_iteration=best_epochs, 
            number_of_cycles=best_cycles, training_percent=best_training), Optim.minimum(result)
end

# Choose your approach and run it:

println("Choose your hyperparameter optimization approach:")
println("1. Smart Random Search (Recommended - most reliable)")
println("2. Optim.jl Continuous Optimization (More sophisticated)")

# For now, let's run the smart random search
best_params, best_loss, all_results = smart_random_hyperparameter_search(10)

# Test the best parameters with your full function
println("\nTesting best parameters with full optimization function...")
final_result = optimizeBlackHole(
    learningRate = best_params.learning_rate,
    epochsPerIteration = best_params.epochs_per_iteration,
    numberOfCycles = best_params.number_of_cycles,
    totalTrainingPercent = best_params.training_percent,
    true_parameters = [10, 0.2],
    initial_guess = [10, 0.2]
)

println("Verification complete!")