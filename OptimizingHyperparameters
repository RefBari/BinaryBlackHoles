using LinearAlgebra
using Distributions
using Plots
using DifferentialEquations
using LaTeXStrings
using Optimization, OptimizationOptimJL
using Optimisers
using OptimizationOptimisers
using Printf
using Plots.Measures
using Lux
using ComponentArrays
using Random
using ForwardDiff

include("/Users/rbari/Downloads/common_denominator-2020_scimlforbbhs-e19e6808aad2/paper/zenodo/GWaveInversion/SchwarzschildTrainingData")

losses = []
partition_boundaries = []
final_paramaters = []
solutions_list = []
parameters_list = []
final_predicted_h_plus = []
training_h_plus_wave = []

function optimizeBlackHole(; learningRate, epochsPerIteration, numberOfCycles, totalTrainingPercent, true_parameters, initial_guess)
   
    global losses = []
    global partition_boundaries = []
    global final_paramaters = []
    global solutions_list = []
    global parameters_list = []
    global final_predicted_h_plus = []
    global training_h_plus_wave = []

    trainingFraction = totalTrainingPercent # What total fraction of the training data will the neural network learn from?
    p_guess = pe_2_EL(initial_guess[1], initial_guess[2]) # Uses the pe_2_EL function to convert initial guess to (M = 1, E, L)
    
    true_p = true_parameters[1] # True semi-latus rectum
    true_e = true_parameters[2] # True eccentricity

    # This trainingData returns [prob, true_sol, h_plus_training, h_cross_training]
    trainingData = create_Schwarzschild_trainingData([true_p, true_e]) # Generate Training Data (Gravitational Waveforms)

    timestep = 1 # Timestep for ODE Solver and Optimizer

    true_solution = trainingData[2] # True Solution

    function SchwarzschildHamiltonian_GENERIC(du, u, p, t)
        x = u # u[1] = t, u[2] = r, u[3] = Œ∏, u[4] = œï  
        NN_params = p.NN
        M, E, L = p.parameters.M, p.parameters.E, p.parameters.L

        function H(state_vec)
            t, r, Œ∏, œÜ, p_t, p_r, p_Œ∏, p_œÜ = state_vec

            H_kepler = p_r^2/2 - M/r + p_œÜ^2/(2*r^2)

            NN_correction = NN([r, p_r, p_œÜ, p_t], NN_params, NN_state)[1][1]

            return H_kepler + NN_correction # Returns equations of motion in PROPER time
        end
        
        # Compute gradient using ForwardDiff
        grad_H = ForwardDiff.gradient(H, x)
        
        # Define symplectic matrix L (8x8)
        J = [zeros(4,4)  I(4);
             -I(4)       zeros(4,4)]
        
        # Hamilton's equations: ·∫ã = J*‚àáH
        du_dœÑ = J * grad_H

        t_val, r_val = x[1], x[2]
        f_val = 1 - 2*M/r_val
        dœÑ_dt = f_val/E

        du .= du_dœÑ .* dœÑ_dt # du / dt Returns equations of motion in COORDINATE time
        du[1] = 1/dœÑ_dt
    end

    # Neural network setup 
    NN = Chain(Dense(4, 4, tanh), # Learns correction term in terms of 4 parameters: r, p_t, p_r, p_œï
            Dense(4, 4, tanh),
            Dense(4, 1))        # Output 1 correction instead of 3

    rng = MersenneTwister(222)
    NN_params, NN_state = Lux.setup(rng, NN)
    precision = Float64
    NN_params = Lux.fmap(x -> precision.(x), NN_params)

    # Same weight initialization as your original
    for layer in NN_params
        if ~isempty(layer)
            layer.weight .*= 0 .* layer.weight .+ precision(1e-2) * (randn(rng, eltype(layer.weight), size(layer.weight)))
            layer.bias .*= 0 .* layer.bias .+ precision(1e-2) * (randn(rng, eltype(layer.bias), size(layer.bias)))
        end
    end

    # What will the neural network learn? The following parameters: M, E, L, NN_param
    R = initial_guess[1]/(1+initial_guess[2])
    M = p_guess[1]
    E = p_guess[2]
    L = p_guess[3]
    BH_SlightPush = 0
    
    Œ∏ = (; NN = NN_params, parameters = (M = M, E = E, L = L))
    Œ∏ = ComponentVector{precision}(Œ∏);

    # Initial State Vector for Particle orbiting Schwarzschild BH
    u0 = [0, R, pi/2, 0, -1*E, 0, 0, L] # u = [t‚ÇÄ, r‚ÇÄ, Œ∏‚ÇÄ, œï‚ÇÄ, p‚Çú‚ÇÄ, p·µ£‚ÇÄ, p_Œ∏‚ÇÄ, p_œï‚ÇÄ]
    tspan = (0.0, 2000.0)
    t = 0:timestep:2000.0

    # Define the ODE Problem using the Neural Network
    prob_learn = ODEProblem(SchwarzschildHamiltonian_GENERIC, u0, tspan, Œ∏)

    function evaluateHamiltonian(u)
        M = 1
        r = u[2]
        f = 1-(2*M/r)
        p‚Çú = u[5]
        p·µ£ = u[6]
        p_œï = u[8]
        H = (1/2) * (-f^(-1)*p‚Çú^2 + f*(p·µ£)^2 + (1/r^2)*(p_œï)^2)
        return H
    end

    function loss(pn, trainingFraction)
        newprob = remake(prob_learn, p = pn)
        sol = solve(newprob, RK4(), saveat=timestep)
        
        predicted_waveform_plus = compute_waveform(timestep, sol, 1.0; coorbital=false)[1]
        predicted_waveform_cross = compute_waveform(timestep, sol, 1.0; coorbital=false)[2]
        
        h_plus_training = trainingData[3]
        h_cross_training = trainingData[4]

        # Compare only the overlapping portion
        n_train = Int(floor(length(h_plus_training)*trainingFraction))
        n_pred = length(predicted_waveform_plus)
        n_compare = min(n_pred, n_train)
        
        loss_value = sum(abs2, predicted_waveform_plus[1:n_compare] .- h_plus_training[1:n_compare])
        loss_value += sum(abs2, predicted_waveform_cross[1:n_compare] .- h_cross_training[1:n_compare])
        loss_value /= n_compare

        H_violations = []
        for state in sol.u  # Loop through all state vectors
            H = evaluateHamiltonian(state)
            push!(H_violations, (H - (-0.5))^2)  # H should equal -1/2
        end
        
        hamiltonian_penalty = mean(H_violations)
        Œª_H = 0  # Weight for Hamiltonian penalty
        
        loss_value += Œª_H * hamiltonian_penalty
        println("Training with fraction: ", trainingFraction, ", n_compare: ", n_compare, ", loss: ", loss_value)
        return loss_value
    end
    
    function callback(pn, loss; dotrain = true)
        if dotrain
            push!(losses, loss);
            @printf("Epoch: %d, Loss: %15.12f \n",length(losses),loss);
            p = plot(losses, label = "Loss", xlabel = "Epochs", ylabel = "Loss", top_margin = 10mm, bottom_margin = 10mm, left_margin = 10mm, right_margin = 10mm)
            vline!(partition_boundaries, label = "Partition")
            display(p)
        end
        return false
    end

    # define Optimization Problem
    adtype = Optimization.AutoFiniteDiff() # instead of Optimization.AutoZygote(), use finite differences
    optf = Optimization.OptimizationFunction((x, p) -> loss(x, trainingFraction), adtype)
    Œ∏_init = Œ∏;

    # create Optimization Problem (function + initial guess for parameters)
    optprob = Optimization.OptimizationProblem(optf, Œ∏_init)

    # choose method for solving problem 
    lr = learningRate;

    # solve the problem
    num_iters = epochsPerIteration; # number of iterations per partition (i.e., 2 partitions means one run + 2 additional runs = 3 runs * 25 epochs/run = 75 epochs)
    opt_result = Optimization.solve(optprob, Optim.BFGS(; initial_stepnorm = lr), callback=callback, maxiters=num_iters)
    Œ∏_final = opt_result.u

    # M_final = Œ∏_final.M
    # E_final = Œ∏_final.E
    # L_final = Œ∏_final.L

    NN_params_final = Œ∏_final.NN
    
    newprob = remake(prob_learn, p = Œ∏_final)
    sol = solve(newprob, Tsit5(), saveat=timestep)
    push!(solutions_list, sol)

    h_plus_pred = compute_waveform(timestep, sol, 1.0; coorbital=false)[1]
    h_cross_pred = compute_waveform(timestep, sol, 1.0; coorbital=false)[2]

    # Handle all cases properly
    h_plus_training = trainingData[3]
    h_cross_training = trainingData[4]

    n_pred = length(h_plus_pred)
    n_train = length(h_plus_training)

    if n_pred == n_train
        # Same length - no padding needed
        h_plus_pred_plot = h_plus_pred
    elseif n_pred < n_train
        # Predicted is shorter - pad with zeros
        h_plus_pred_plot = [h_plus_pred; zeros(n_train - n_pred)]
    else  # n_pred > n_train
        # Predicted is longer - truncate
        h_plus_pred_plot = h_plus_pred[1:n_train]
    end

    t_plot = (0:n_train-1) * timestep

    p = plot(t_plot, h_plus_training, label="h+ true", linewidth=2,
        xlabel="Time (s)", ylabel="h+ Amplitude",
        legend=:topright, grid=true, top_margin = 10mm, bottom_margin = 10mm, left_margin = 10mm, right_margin = 10mm)
    plot!(t_plot, h_plus_pred_plot, label="h+ predicted", linewidth=2)# plot!(t_plot, predicted_waveform_plus_old_padded, label="h+ initial prediction", linewidth=2)
    display(p)

    function partitionTraining(numCycles, totalTrainingFraction)
        global partition_boundaries, losses, final_paramaters, solutions_list, parameters_list
        
        amountTrain = totalTrainingFraction / numCycles
        p_final_array = [Œ∏_final]
    
        for i in 1:numCycles
            trainingFraction = i * amountTrain
            push!(partition_boundaries, length(losses))
            push!(final_paramaters, p_final_array[end])
            optf = Optimization.OptimizationFunction((x, p) -> loss(x, trainingFraction), adtype)
            
            Œ∏_current = p_final_array[end]
            optprob = Optimization.OptimizationProblem(optf, Œ∏_current)
            opt_result_2 = Optimization.solve(optprob, Optim.BFGS(; initial_stepnorm = lr), callback=callback, maxiters=num_iters)
    
            Œ∏_final_2 = opt_result_2.u;
            push!(p_final_array, Œ∏_final_2)
            newprob_2 = remake(prob_learn, p = Œ∏_final_2)
            sol_2 = solve(newprob_2, Tsit5(), saveat=timestep)
            push!(solutions_list, sol_2)
            push!(parameters_list, getParameters(sol_2))
    
            h_plus_pred = compute_waveform(timestep, sol_2, 1.0; coorbital=false)[1]
            h_cross_pred = compute_waveform(timestep, sol_2, 1.0; coorbital=false)[2]
    
            # Handle all cases properly
            n_pred = length(h_plus_pred)
            n_train = length(h_plus_training)
    
            if n_pred == n_train
                # Same length - no padding needed
                h_plus_pred_plot = h_plus_pred
            elseif n_pred < n_train
                # Predicted is shorter - pad with zeros
                h_plus_pred_plot = [h_plus_pred; zeros(n_train - n_pred)]
            else  # ADD THIS: when n_pred > n_train
                h_plus_pred_plot = h_plus_pred[1:n_train]
            end
            
            t_plot = (0:n_train-1) * timestep
    
            p = plot(t_plot, h_plus_training, color = "lightsalmon", label="h+ true", linewidth=2,
            xlabel="Time (s)", ylabel="h+ Amplitude",
            legend=:topright, grid=true, bottom_margin = 10mm, top_margin = 10mm, left_margin = 10mm, right_margin = 10mm)
            plot!(t_plot, h_plus_pred_plot, label="h+ predicted",
                    color = "blue")# plot!(t_plot, predicted_waveform_plus_old_padded, label="h+ initial prediction", linewidth=2)
            display(p)
            push!(final_predicted_h_plus, h_plus_pred_plot)
            push!(training_h_plus_wave, h_plus_training)
        end
    end    

    numCycles = numberOfCycles

    partitionTraining(numCycles, trainingFraction)

    x = range(6, 12, length=20)
    y = (x .- 6) ./ 2
    p = plot(x, y, ylims=(-0.1, 1), xlims = (6, 12), linewidth = 3, bottom_margin = 10mm, top_margin = 10mm, left_margin = 10mm, right_margin = 10mm, label = "Separatrix", xlabel = "p (Semi-latus Rectum)", ylabel = "e (Eccentricity)", legend=:bottomright)
    scatter!([getParameters(true_solution)[1]], [getParameters(true_solution)[2]], color = "lightsalmon", markersize = 5, label = "True Parameters")
    
    for i in 1:numCycles
        scatter!([parameters_list[i][1]], [parameters_list[i][2]], color = "darkseagreen1", markersize = 3, legend = false)
    end
    
    display(p)

    return (parameters_list[end][1] - getParameters(true_solution)[1])^2 + (parameters_list[end][2] - getParameters(true_solution)[2])^2
end

function objective_function(learningRate, epochsPerIteration, numberOfCycles, totalTrainingPercent)
    try
        parameter_error = optimizeBlackHole(
            learningRate = learningRate,
            epochsPerIteration = epochsPerIteration, 
            numberOfCycles = numberOfCycles,
            totalTrainingPercent = totalTrainingPercent,
            true_parameters = [10, 0.2],
            initial_guess = [10, 0.2]
        )
        println("lr=$learningRate, epochs=$epochsPerIteration ‚Üí error=$parameter_error")
        return parameter_error
    catch e
        println("Failed: $e")
        return 100.0
    end
end

# Use ONLY discrete choices to avoid any distribution conflicts
ho = @hyperopt for i=20,
    learningRate = [1e-3, 3e-3, 6e-3, 1e-2, 2e-2],      # Pick from list
    epochsPerIteration = [2, 5, 10, 20, 50],             # Pick from list
    numberOfCycles = [3, 5, 7, 10, 15],                  # Pick from list  
    totalTrainingPercent = [0.1, 0.17, 0.3, 0.5, 0.7]   # Pick from list
    
    objective_function(learningRate, epochsPerIteration, numberOfCycles, totalTrainingPercent)
end

println("üèÜ BEST PARAMETERS:")
println("Learning Rate: $(ho.minimizer[1])")
println("Epochs: $(ho.minimizer[2])")  
println("Cycles: $(ho.minimizer[3])")
println("Training %: $(ho.minimizer[4])")
println("Best Error: $(ho.minimum)")

# Run with best parameters
optimizeBlackHole(
    learningRate = ho.minimizer[1],
    epochsPerIteration = ho.minimizer[2], 
    numberOfCycles = ho.minimizer[3],
    totalTrainingPercent = ho.minimizer[4],
    true_parameters = [10, 0.2],
    initial_guess = [10, 0.2]
)