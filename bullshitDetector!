using Plots
using DifferentialEquations
using LaTeXStrings
using Optimization, OptimizationOptimJL
using Optimisers
using OptimizationOptimisers
using Printf
using Plots.Measures

include("/Users/rbari/Downloads/common_denominator-2020_scimlforbbhs-e19e6808aad2/paper/zenodo/GWaveInversion/SchwarzschildTrainingData")

function pe_2_EL(semilatusRectum, eccentricity)
    p = semilatusRectum
    e = eccentricity
    
    M = 1
    E = sqrt( (p-2-2*e)*(p-2+2*e) / (p*(p-3-e^2)) )
    L = sqrt( (p^2 * M^2) / (p-3-e) )
  
    return [M, E, L]
end

function getParameters(solution)
    # ADD SAFETY CHECK: Make sure solution has enough data
    if size(solution, 2) < 10  # Need at least 10 time points
        println("Warning: Solution too short for parameter extraction")
        return 0.0, 0.0  # Return dummy values
    end
    
    try
        r_max = maximum(solution[2,:])
        r_min = minimum(solution[2,:])
        e_true = (r_max-r_min)/(r_max+r_min) # True Eccentricity
        p_true = (2*r_max*r_min)/(r_max+r_min) # True semi-latus rectum
        return p_true, e_true
    catch e
        println("Error in getParameters: $e")
        return 0.0, 0.0  # Return dummy values
    end
end 

# Global variables
losses = []
partition_boundaries = []
final_paramaters = []
solutions_list = []
parameters_list = []

function optimizeBlackHole(; learningRate, epochsPerIteration, numberOfCycles, totalTrainingPercent, initial_guess)
    p_guess = initial_guess
    trainingFraction = totalTrainingPercent

    trainingData = create_Schwarzschild_trainingData([10, 0.5])
    true_problem = trainingData[1]
    true_solution = solve(true_problem, Tsit5(), saveat = 240)
    
    println("Training data setup complete")

    function loss(pn, trainingFraction)
        try
            # ADD PARAMETER BOUNDS CHECK
            if length(pn) != 3
                println("Warning: Wrong parameter vector length: $(length(pn))")
                return 1e6
            end
            
            # Check for reasonable parameter values
            if any(isnan.(pn)) || any(isinf.(pn))
                println("Warning: Invalid parameter values: $pn")
                return 1e6
            end
            
            newprob = remake(true_problem, p = pn)
            sol = solve(newprob, Tsit5(), saveat=240, maxiters=1e6)
            
            # CHECK SOLUTION VALIDITY
            if sol.retcode != :Success
                println("Warning: ODE solution failed with retcode: $(sol.retcode)")
                return 1e6
            end
            
            if length(sol.t) < 10
                println("Warning: Solution too short: $(length(sol.t)) points")
                return 1e6
            end
            
            predicted_waveform_plus = compute_waveform(240, sol, 1.0)[1]
            predicted_waveform_cross = compute_waveform(240, sol, 1.0)[2]
            
            # CHECK WAVEFORM VALIDITY
            if length(predicted_waveform_plus) < 5 || length(predicted_waveform_cross) < 5
                println("Warning: Predicted waveform too short")
                return 1e6
            end
            
            h_plus_training = trainingData[2]
            h_cross_training = trainingData[3]

            # Compare only the overlapping portion
            n_train = Int(floor(length(h_plus_training)*trainingFraction))
            n_pred = length(predicted_waveform_plus)
            n_compare = min(n_pred, n_train)
            
            if n_compare <= 0
                println("Warning: No points to compare")
                return 1e6
            end
            
            loss_value = sum(abs2, predicted_waveform_plus[1:n_compare] .- h_plus_training[1:n_compare])
            loss_value += sum(abs2, predicted_waveform_cross[1:n_compare] .- h_cross_training[1:n_compare])
            loss_value /= n_compare
            
            # Check for invalid loss
            if isnan(loss_value) || isinf(loss_value)
                println("Warning: Invalid loss value: $loss_value")
                return 1e6
            end
            
            println("Training with fraction: ", trainingFraction, ", n_compare: ", n_compare, ", loss: ", loss_value)
            return loss_value
            
        catch e
            println("Error in loss function: $e")
            println("Parameters were: $pn")
            return 1e6
        end
    end    
    
    function callback(pn, loss; dotrain = true)
        if dotrain
            push!(losses, loss);
            @printf("Epoch: %d, Loss: %15.12f \n",length(losses),loss);
            try
                p = plot(losses, label = "Loss", xlabel = "Epochs", ylabel = "Loss", 
                        top_margin = 10mm, bottom_margin = 10mm, left_margin = 10mm, right_margin = 10mm)
                vline!(partition_boundaries, label = "Partition")
                display(p)
            catch e
                println("Error in plotting callback: $e")
            end
        end
        return false
    end

    # define Optimization Problem
    adtype = Optimization.AutoFiniteDiff()
    optf = Optimization.OptimizationFunction((x, p) -> loss(x, trainingFraction), adtype)
    optprob = Optimization.OptimizationProblem(optf, p_guess)

    # choose method for solving problem 
    lr = learningRate;
    opt_method = Optimisers.Adam(lr)

    # solve the problem
    num_iters = epochsPerIteration;
    println("Starting optimization...")
    opt_result = Optimization.solve(optprob, opt_method, callback=callback, maxiters=num_iters)
    p_final = opt_result.u
    
    println("Initial optimization complete")

    # SAFE SOLUTION GENERATION
    try
        newprob = remake(true_problem, p = p_final)
        sol = solve(newprob, Tsit5(), saveat=240)
        
        if sol.retcode == :Success && length(sol.t) > 10
            push!(solutions_list, sol)
            h_plus_pred = compute_waveform(240, sol, 1.0)[1]
            h_cross_pred = compute_waveform(240, sol, 1.0)[2]

            h_plus_training = trainingData[2]
            h_cross_training = trainingData[3]

            n_pred = length(h_plus_pred)
            n_train = length(h_plus_training)

            if n_pred == n_train
                h_plus_pred_plot = h_plus_pred
            elseif n_pred < n_train
                h_plus_pred_plot = [h_plus_pred; zeros(n_train - n_pred)]
            else
                h_plus_pred_plot = h_plus_pred[1:n_train]
            end

            t_plot = (0:n_train-1) * 240

            p = plot(t_plot, h_plus_training, label="h+ true", linewidth=2,
                xlabel="Time (s)", ylabel="h+ Amplitude",
                legend=:topright, grid=true, top_margin = 10mm, bottom_margin = 10mm, left_margin = 10mm, right_margin = 10mm)
            plot!(t_plot, h_plus_pred_plot, label="h+ predicted", linewidth=2)
            display(p)
        else
            println("Warning: Final solution generation failed")
        end
    catch e
        println("Error generating final solution: $e")
    end

    function partitionTraining(numCycles, totalTrainingFraction)
        global partition_boundaries, losses, final_paramaters, solutions_list, parameters_list
        
        amountTrain = totalTrainingFraction / numCycles
        p_final_array = [p_final]
    
        for i in 1:numCycles
            println("Starting partition $i of $numCycles...")
            trainingFraction = i * amountTrain
            push!(partition_boundaries, length(losses))
            push!(final_paramaters, p_final_array[end])
            
            try
                optf = Optimization.OptimizationFunction((x, p) -> loss(x, trainingFraction), adtype)
                optprob = Optimization.OptimizationProblem(optf, p_final_array[end])
                opt_result_2 = Optimization.solve(optprob, opt_method, callback = callback, maxiters = num_iters)
        
                p_final_2 = opt_result_2.u
                push!(p_final_array, p_final_2)
                
                newprob_2 = remake(true_problem, p = p_final_2)
                sol_2 = solve(newprob_2, Tsit5(), saveat=240)
                
                # CHECK SOLUTION BEFORE USING
                if sol_2.retcode == :Success && length(sol_2.t) > 10
                    push!(solutions_list, sol_2)
                    push!(parameters_list, getParameters(sol_2))
            
                    h_plus_pred = compute_waveform(240, sol_2, 1.0)[1]
                    h_cross_pred = compute_waveform(240, sol_2, 1.0)[2]
            
                    h_plus_training = trainingData[2]
                    h_cross_training = trainingData[3]
                    
                    n_pred = length(h_plus_pred)
                    n_train = length(h_plus_training)
            
                    if n_pred == n_train
                        h_plus_pred_plot = h_plus_pred
                    elseif n_pred < n_train
                        h_plus_pred_plot = [h_plus_pred; zeros(n_train - n_pred)]
                    else
                        h_plus_pred_plot = h_plus_pred[1:n_train]
                    end
                    
                    t_plot = (0:n_train-1) * 240
            
                    p = plot(t_plot, h_plus_training, label="h+ true", linewidth=2,
                            xlabel="Time (s)", ylabel="h+ Amplitude",
                            legend=:topright, grid=true, bottom_margin = 10mm, top_margin = 10mm, left_margin = 10mm, right_margin = 10mm)
                    plot!(t_plot, h_plus_pred_plot, label="h+ predicted", linewidth=2)
                    display(p)
                else
                    println("Warning: Partition $i solution failed, using fallback")
                    push!(solutions_list, true_solution)
                    push!(parameters_list, getParameters(true_solution))
                end
                
            catch e
                println("Error in partition $i: $e")
                push!(solutions_list, true_solution)
                push!(parameters_list, getParameters(true_solution))
            end
        end
    end    

    numCycles = numberOfCycles
    partitionTraining(numCycles, trainingFraction)

    # SAFE FINAL PLOTTING
    try
        x = range(6, 12, length=20)
        y = (x .- 6) ./ 2
        p = plot(x, y, ylims=(-0.1, 1), xlims = (6, 12), linewidth = 3, 
                bottom_margin = 10mm, top_margin = 10mm, left_margin = 10mm, right_margin = 10mm, 
                label = "Separatrix", xlabel = "p (Semi-latus Rectum)", ylabel = "e (Eccentricity)", 
                legend=:bottomright)
        
        true_params = getParameters(true_solution)
        scatter!([true_params[1]], [true_params[2]], color = "lightsalmon", markersize = 5, label = "True Parameters")
        
        # SAFE PARAMETER PLOTTING
        for i in 1:min(length(parameters_list), numCycles)
            if i <= length(parameters_list) && length(parameters_list[i]) >= 2
                scatter!([parameters_list[i][1]], [parameters_list[i][2]], color = "darkseagreen1", markersize = 3, legend = false)
            end
        end
        
        display(p)

        if length(parameters_list) > 0 && length(parameters_list[end]) >= 2
            final_error = (parameters_list[end][1] - true_params[1])^2 + (parameters_list[end][2] - true_params[2])^2
            println("Final error: $final_error")
            return final_error
        else
            println("Warning: No valid final parameters found")
            return 1e6
        end
        
    catch e
        println("Error in final plotting: $e")
        return 1e6
    end
end

# Clear globals
global losses = []
global partition_boundaries = []
global final_paramaters = []
global solutions_list = []
global parameters_list = []

println("Starting robust optimization...")
result = optimizeBlackHole(learningRate = 1e-2, 
                          epochsPerIteration = 25, 
                          numberOfCycles = 3, 
                          totalTrainingPercent = 0.25, 
                          initial_guess = [1, 0.96, 3.9])

println("Optimization complete. Result: $result")