using Plots
using DifferentialEquations
using LaTeXStrings
using Optimisers
using OptimizationOptimisers
using Printf
using Plots.Measures
using OrdinaryDiffEq, DiffEqFlux
using Optimization, OptimizationOptimisers, OptimizationOptimJL
using Lux, ComponentArrays
using Printf
using Random

include("/Users/rbari/Downloads/common_denominator-2020_scimlforbbhs-e19e6808aad2/paper/zenodo/GWaveInversion/SchwarzschildTrainingData")

losses = []
partition_boundaries = []
final_paramaters = []
solutions_list = []
parameters_list = []
final_predicted_h_plus = []
training_h_plus_wave = []

function optimizeBlackHole(; learningRate, epochsPerIteration, numberOfCycles, totalTrainingPercent, true_parameters, initial_guess)
    # Initialize tracking arrays
    global losses, partition_boundaries, parameters_list
    losses = []
    partition_boundaries = []
    parameters_list = []
    
    p_guess = pe_2_EL(initial_guess[1], initial_guess[2])
    trainingFraction = totalTrainingPercent
    
    true_p = true_parameters[1]
    true_e = true_parameters[2]

    trainingData = create_Schwarzschild_trainingData([true_p, true_e])

    true_problem = trainingData[1]
    true_solution = trainingData[2]
    h_plus_training = trainingData[3]
    h_cross_training = trainingData[4]

    # Define u0 and tspan from the training data
    u0 = true_problem.u0
    tspan = true_problem.tspan

    timestep = 1
    
    # Create the neural network that will learn corrections to the flat space equations of motion and learn the geodesic equations of motion
    NN = Chain(Dense(8, 4,tanh),
            Dense(4, 4, tanh),
            Dense(4, 3))
    rng = MersenneTwister(222)
    NN_params, NN_state = Lux.setup(rng, NN)
    precision = Float64
    NN_params = Lux.fmap(x -> precision.(x), NN_params)

    # Make the weights and biases of the neural network zero and add tiny random values
    for layer in NN_params
        if ~isempty(layer)
            layer.weight .*= 0 .* layer.weight .+ precision(1e-6) * (randn(rng, eltype(layer.weight), size(layer.weight)))
            layer.bias .*= 0 .* layer.bias .+ precision(1e-6) * (randn(rng, eltype(layer.bias), size(layer.bias)))
        end
    end

    θ = (; M = p_guess[1], E = p_guess[2], L = p_guess[3], NN = NN_params)
    θ = ComponentVector{precision}(θ);

    function SchwarzschildNN(du, u, p, t)
        coord_time, r, θ, ϕ, p_t, p_r, p_θ, p_ϕ = u
        M = p.M
        E = p.E
        L = p.L

        NN_params = p.NN
        F = NN(u, NN_params, NN_state)[1]

        du[1] = dt = 1 * (1+1e-3*F[1])
        du[2] = dr = p_r * (1+1e-3*F[2])
        du[3] = dθ = 0
        du[4] = dϕ = L/r^2

        du[5] = dp_t = 0
        du[6] = dp_r = (-M/r^2 + L^2/(r^3))*(1+1e-3*F[3])
        du[7] = dp_θ = 0
        du[8] = dp_ϕ = 0
    end

    prob_learn = ODEProblem(SchwarzschildNN, u0, tspan, θ)

    function loss(pn, trainingFraction)
        newprob = remake(prob_learn, p = pn)
        sol = solve(newprob, Tsit5(), saveat=timestep)
        
        predicted_waveform_plus = compute_waveform(timestep, sol, 1.0)[1]
        predicted_waveform_cross = compute_waveform(timestep, sol, 1.0)[2]

        # Compare only the overlapping portion
        n_train = Int(floor(length(h_plus_training)*trainingFraction))
        n_pred = length(predicted_waveform_plus)
        n_compare = min(n_pred, n_train)
        
        loss_value = sum(abs2, predicted_waveform_plus[1:n_compare] .- h_plus_training[1:n_compare])
        loss_value += sum(abs2, predicted_waveform_cross[1:n_compare] .- h_cross_training[1:n_compare])
        loss_value /= n_compare
        println("Training with fraction: ", trainingFraction, ", n_compare: ", n_compare, ", loss: ", loss_value)
        return loss_value
    end
    
    function callback(pn, loss; dotrain = true)
        if dotrain
            push!(losses, loss);
            @printf("Epoch: %d, Loss: %15.12f \n",length(losses),loss);
            p = plot(losses, label = "Loss", xlabel = "Epochs", ylabel = "Loss", top_margin = 10mm, bottom_margin = 10mm, left_margin = 10mm, right_margin = 10mm)
            vline!(partition_boundaries, label = "Partition")
            display(p)
        end
        return false
    end

    # define Optimization Problem
    adtype = Optimization.AutoFiniteDiff() # instead of Optimization.AutoZygote(), use finite differences
    optf = Optimization.OptimizationFunction((x, p) -> loss(x, trainingFraction), adtype)
    
    # create Optimization Problem (function + initial guess for parameters)
    optprob = Optimization.OptimizationProblem(optf, θ)

    # choose method for solving problem 
    lr = learningRate;

    # solve the problem
    num_iters = epochsPerIteration; # number of iterations per partition (i.e., 2 partitions means one run + 2 additional runs = 3 runs * 25 epochs/run = 75 epochs)
    opt_result = Optimization.solve(optprob, Optim.BFGS(; initial_stepnorm = lr), callback=callback, maxiters=num_iters)
    p_final = opt_result.u

    newprob = remake(prob_learn, p = p_final)
    sol = solve(newprob, Tsit5(), saveat=timestep)
    push!(solutions_list, sol)

    h_plus_pred = compute_waveform(timestep, sol, 1.0)[1]
    h_cross_pred = compute_waveform(timestep, sol, 1.0)[2]

    # Handle all cases properly
    n_pred = length(h_plus_pred)
    n_train = length(h_plus_training)

    if n_pred == n_train
        # Same length - no padding needed
        h_plus_pred_plot = h_plus_pred
    elseif n_pred < n_train
        # Predicted is shorter - pad with zeros
        h_plus_pred_plot = [h_plus_pred; zeros(n_train - n_pred)]
    else  # n_pred > n_train
        # Predicted is longer - truncate
        h_plus_pred_plot = h_plus_pred[1:n_train]
    end

    t_plot = (0:n_train-1) * timestep

    p = plot(t_plot, h_plus_training, label="h+ true", linewidth=2,
        xlabel="Time (s)", ylabel="h+ Amplitude",
        legend=:topright, grid=true, top_margin = 10mm, bottom_margin = 10mm, left_margin = 10mm, right_margin = 10mm)
    plot!(t_plot, h_plus_pred_plot, label="h+ predicted", linewidth=2)
    display(p)

    function partitionTraining(numCycles, totalTrainingFraction)
        global partition_boundaries, losses, final_paramaters, solutions_list, parameters_list, final_predicted_h_plus
        
        amountTrain = totalTrainingFraction / numCycles
        p_final_array = [p_final]
    
        for i in 1:numCycles
            trainingFraction = i * amountTrain
            push!(partition_boundaries, length(losses))
            push!(final_paramaters, p_final_array[end])
            optf = Optimization.OptimizationFunction((x, p) -> loss(x, trainingFraction), adtype)
            θ_current = ComponentVector{precision}((; M = p_final_array[end][1], E = p_final_array[end][2], L = p_final_array[end][3], NN = NN_params))
            optprob = Optimization.OptimizationProblem(optf, θ_current)
            opt_result_2 = Optimization.solve(optprob, Optim.BFGS(; initial_stepnorm = lr), callback=callback, maxiters=num_iters)
    
            p_final_2 = opt_result_2.u
            push!(p_final_array, p_final_2)
            newprob_2 = remake(prob_learn, p = p_final_2)
            sol_2 = solve(newprob_2, Tsit5(), saveat=timestep)
            push!(solutions_list, sol_2)
            push!(parameters_list, getParameters(sol_2))
    
            h_plus_pred = compute_waveform(timestep, sol_2, 1.0)[1]
            h_cross_pred = compute_waveform(timestep, sol_2, 1.0)[2]
            
            # Save the predicted waveform
            n_pred = length(h_plus_pred)
            n_train = length(h_plus_training)
    
            if n_pred == n_train
                h_plus_pred_plot = h_plus_pred
            elseif n_pred < n_train
                h_plus_pred_plot = [h_plus_pred; zeros(n_train - n_pred)]
            else
                h_plus_pred_plot = h_plus_pred[1:n_train]
            end
            
            push!(final_predicted_h_plus, h_plus_pred_plot)
            push!(training_h_plus_wave, h_plus_training)
        end
    end    

    numCycles = numberOfCycles
    partitionTraining(numCycles, trainingFraction)

    # Create evolving plots showing training progression
    function plot_training_evolution()
        true_p, true_e = getParameters(true_solution)
        t_plot = (0:length(h_plus_training)-1) * timestep
        
        # Number of snapshots to show (including intermediate epochs)
        n_snapshots = min(6, length(final_predicted_h_plus) + 1)  # +1 for initial
        snapshot_indices = round.(Int, range(1, length(losses), length=n_snapshots))
        
        plots_array = []
        
        for (i, epoch_idx) in enumerate(snapshot_indices)
            # Plot 1: Loss evolution up to this point
            p1 = plot(losses[1:epoch_idx], label="Loss", xlabel="Epochs", ylabel="Loss",
                     title="Loss (Epoch $epoch_idx)", linewidth=2, 
                     xlims=(1, length(losses)), ylims=(minimum(losses)*0.9, maximum(losses)*1.1))
            vline!(p1, partition_boundaries[partition_boundaries .<= epoch_idx], label="Partition", alpha=0.5)
            
            # Plot 2: Waveform fit at this epoch
            if i == 1
                # Initial prediction (before training)
                newprob_init = remake(prob_learn, p = θ)
                sol_init = solve(newprob_init, Tsit5(), saveat=timestep)
                h_plus_init = compute_waveform(timestep, sol_init, 1.0)[1]
                n_compare = min(length(h_plus_init), length(h_plus_training), 1000)
                p2 = plot(t_plot[1:n_compare], h_plus_training[1:n_compare], label="True", linewidth=2)
                plot!(p2, t_plot[1:n_compare], h_plus_init[1:n_compare], label="Initial", linewidth=2)
                title!(p2, "Waveform (Initial)")
            else
                # Use saved predictions
                pred_idx = min(i-1, length(final_predicted_h_plus))
                n_compare = min(length(final_predicted_h_plus[pred_idx]), length(h_plus_training), 1000)
                p2 = plot(t_plot[1:n_compare], h_plus_training[1:n_compare], label="True", linewidth=2)
                plot!(p2, t_plot[1:n_compare], final_predicted_h_plus[pred_idx][1:n_compare], label="Predicted", linewidth=2)
                title!(p2, "Waveform (Epoch $epoch_idx)")
            end
            xlabel!(p2, "Time")
            ylabel!(p2, "h+")
            
            # Plot 3: Parameter space evolution
            x = range(6, 12, length=20)
            y = (x .- 6) ./ 2
            p3 = plot(x, y, label="Separatrix", xlabel="p", ylabel="e", 
                     title="Parameters (Epoch $epoch_idx)", linewidth=2,
                     ylims=(-0.1, 1), xlims=(6, 12))
            scatter!(p3, [true_p], [true_e], color="red", markersize=6, label="True")
            
            # Show parameter evolution up to this point
            if i > 1
                param_idx = min(i-1, length(parameters_list))
                for j in 1:param_idx
                    alpha_val = j / param_idx  # Fade from transparent to opaque
                    scatter!(p3, [parameters_list[j][1]], [parameters_list[j][2]], 
                            color="blue", alpha=alpha_val, markersize=4, label="")
                end
                # Highlight current point
                scatter!(p3, [parameters_list[param_idx][1]], [parameters_list[param_idx][2]], 
                        color="blue", markersize=6, label="Current")
            end
            
            # Combine plots for this snapshot
            l = @layout [a b c]
            combined = plot(p1, p2, p3, layout=l, size=(1200, 400))
            push!(plots_array, combined)
        end
        
        # Display all snapshots
        for (i, p) in enumerate(plots_array)
            println("Training Evolution - Snapshot $i/$(length(plots_array))")
            display(p)
            if i < length(plots_array)
                sleep(1)  # Pause between snapshots
            end
        end
    end
    
    # Show the evolution
    plot_training_evolution()

    # Return final error
    return (parameters_list[end][1] - getParameters(true_solution)[1])^2 + (parameters_list[end][2] - getParameters(true_solution)[2])^2
end

optimizeBlackHole(learningRate = 1e-3, 
                  epochsPerIteration = 3, 
                  numberOfCycles = 2, 
                  totalTrainingPercent = 0.1, 
                  true_parameters = [10, 0.1], # Create training data for these (p_0, e_0) values
                  initial_guess = [10.2, 0.2]) # Take this initial (p, e) guess